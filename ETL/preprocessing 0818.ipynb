{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 범주형 데이터 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import platform\n",
    "from matplotlib import font_manager,rc\n",
    "\n",
    "if platform.system()=='Darwin':\n",
    "\trc('font',family='AppleGothic')\n",
    "elif platform.system()=='Windows':\n",
    "\tfont_name=font_manager.FontProperties(fname='c:/windows/Fonts/malgun.ttf').get_name()\n",
    "\trc('font',family=font_name)\n",
    "\n",
    "\t\n",
    "# 그래프에 음수를 사용하기 위한 설정\n",
    "plt.rcParams['axes.unicode_minus']=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model year</th>\n",
       "      <th>origin</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>chevrolet chevelle malibu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693.0</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>buick skylark 320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>plymouth satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>amc rebel sst</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>ford torino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>27.0</td>\n",
       "      <td>4</td>\n",
       "      <td>140.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>2790.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>ford mustang gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>44.0</td>\n",
       "      <td>4</td>\n",
       "      <td>97.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>24.6</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>vw pickup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>32.0</td>\n",
       "      <td>4</td>\n",
       "      <td>135.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>2295.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>dodge rampage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>28.0</td>\n",
       "      <td>4</td>\n",
       "      <td>120.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2625.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>ford ranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>31.0</td>\n",
       "      <td>4</td>\n",
       "      <td>119.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2720.0</td>\n",
       "      <td>19.4</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>chevy s-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0    18.0          8         307.0       130.0  3504.0          12.0   \n",
       "1    15.0          8         350.0       165.0  3693.0          11.5   \n",
       "2    18.0          8         318.0       150.0  3436.0          11.0   \n",
       "3    16.0          8         304.0       150.0  3433.0          12.0   \n",
       "4    17.0          8         302.0       140.0  3449.0          10.5   \n",
       "..    ...        ...           ...         ...     ...           ...   \n",
       "393  27.0          4         140.0        86.0  2790.0          15.6   \n",
       "394  44.0          4          97.0        52.0  2130.0          24.6   \n",
       "395  32.0          4         135.0        84.0  2295.0          11.6   \n",
       "396  28.0          4         120.0        79.0  2625.0          18.6   \n",
       "397  31.0          4         119.0        82.0  2720.0          19.4   \n",
       "\n",
       "     model year  origin                       name  \n",
       "0            70       1  chevrolet chevelle malibu  \n",
       "1            70       1          buick skylark 320  \n",
       "2            70       1         plymouth satellite  \n",
       "3            70       1              amc rebel sst  \n",
       "4            70       1                ford torino  \n",
       "..          ...     ...                        ...  \n",
       "393          82       1            ford mustang gl  \n",
       "394          82       2                  vw pickup  \n",
       "395          82       1              dodge rampage  \n",
       "396          82       1                ford ranger  \n",
       "397          82       1                 chevy s-10  \n",
       "\n",
       "[392 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auto_mpg=pd.read_csv('./data 4/auto-mpg.csv',header=None)\n",
    "auto_mpg.columns=['mpg','cylinders','displacement','horsepower','weight','acceleration','model year','origin','name']\n",
    "\n",
    "# horsepower 열의 자료형을 실수로 변경\n",
    "# ?를 None으로 치환하고 제거한 후 자료형 변경\n",
    "auto_mpg['horsepower'].replace('?',np.nan,inplace=True)\n",
    "auto_mpg.dropna(subset=['horsepower'],axis=0,inplace=True)\n",
    "auto_mpg['horsepower']=auto_mpg['horsepower'].astype(\"float\")\n",
    "auto_mpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>저출력</th>\n",
       "      <th>보통출력</th>\n",
       "      <th>고출력</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>392 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     저출력  보통출력  고출력\n",
       "0      0     1    0\n",
       "1      0     1    0\n",
       "2      0     1    0\n",
       "3      0     1    0\n",
       "4      0     1    0\n",
       "..   ...   ...  ...\n",
       "393    1     0    0\n",
       "394    1     0    0\n",
       "395    1     0    0\n",
       "396    1     0    0\n",
       "397    1     0    0\n",
       "\n",
       "[392 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원 핫 인코딩\n",
    "\n",
    "# horsepower 특성을 범주형으로 추가 - 3개의 영역으로 구분\n",
    "\n",
    "# 3개의 구간으로 구분해서 개수와 경계값을 리턴 받아서 저장\n",
    "count,bin_dividers=np.histogram(auto_mpg['horsepower'],bins=3)\n",
    "\n",
    "# 범주형의 형태로 생성\n",
    "\n",
    "auto_mpg['hp_bin']=pd.cut(x=auto_mpg['horsepower'],bins=bin_dividers,labels=['저출력','보통출력','고출력'],include_lowest=True)\n",
    "\n",
    "# one-hot encoding\n",
    "horsepower_dummies=pd.get_dummies(auto_mpg['hp_bin'])\n",
    "horsepower_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " ...\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]]\n",
      "['고출력' '보통출력' '저출력']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "one_hot=LabelBinarizer()\n",
    "print(one_hot.fit_transform(auto_mpg[['hp_bin']]))\n",
    "\n",
    "# 이름을 확인\n",
    "print(one_hot.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 1]\n",
      " [1 1 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 1]\n",
      " [0 0 1 1 0 0 0 0]]\n",
      "['C#' 'C++' 'GO' 'R' 'java' 'kotilin' 'objective-c' 'python']\n"
     ]
    }
   ],
   "source": [
    "# 2개 이상의 특성을 가지고 원 핫 인코딩\n",
    "# 2개 이상의 1이 등장할 수 있다. \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "multi_feature=[('java','python'),('C++','C#'),('C++','kotilin'),('objective-c','python'),('GO','R')]\n",
    "one_hot=MultiLabelBinarizer()\n",
    "print(one_hot.fit_transform(multi_feature))\n",
    "\n",
    "print(one_hot.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>encoder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>저조</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>보통</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>보통</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>저조</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>우수</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>매우우수</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Score  encoder\n",
       "0    저조        1\n",
       "1    보통        2\n",
       "2    보통        2\n",
       "3    저조        1\n",
       "4    우수        3\n",
       "5  매우우수        4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 순서가 의미를 갖는 경우 - replace 함수 이용\n",
    "df=pd.DataFrame({\"Score\":['저조','보통','보통','저조','우수','매우우수']})\n",
    "scale_mapper={\"저조\":1,\"보통\":2,'우수':3,'매우우수':4}\n",
    "df['encoder']=df['Score'].replace(scale_mapper)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [2. 2.]\n",
      " [0. 1.]]\n",
      "[array(['high', 'low', 'normal'], dtype='<U21'), array(['10', '15', '20'], dtype='<U21')]\n"
     ]
    }
   ],
   "source": [
    "# 2개 이상의 인코딩\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "features=np.array([['low',10],['normal',20],['high',15]])\n",
    "\n",
    "ordinal_encoder=OrdinalEncoder()\n",
    "print(ordinal_encoder.fit_transform(features))\n",
    "print(ordinal_encoder.categories_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결측치 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n",
      "[[ 1.    0.87  0.31]\n",
      " [ 0.   -0.67  0.22]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.  ,  0.87,  0.31],\n",
       "       [ 0.  , -0.67,  0.22],\n",
       "       [ 0.  ,  2.1 ,  1.  ],\n",
       "       [ 1.  ,  1.18,  1.34],\n",
       "       [ 0.  ,  1.21,  2.  ],\n",
       "       [ 1.  ,  2.  ,  1.34]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류 모델을 이용한 결측값 대체\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 훈련 데이터 생성\n",
    "X=np.array([[0,2.10,1],[1,1.18,1.34],[0,1.21,2],[1,2,1.34]])\n",
    "\n",
    "# 예측에 사용할 데이터\n",
    "X_with_nan=np.array([[np.nan,0.87,0.31],[np.nan,-0.67,0.22]])\n",
    "\n",
    "clf=KNeighborsClassifier(3,weights='distance')\n",
    "# 첫번째 데이터를 label로 하고 나머지 데이터를 feature 로 설정해서 훈련한다. \n",
    "trained_model=clf.fit(X[:,1:],X[:,0])\n",
    "# 예측 \n",
    "imputed_values=trained_model.predict(X_with_nan[:,1:])\n",
    "print(imputed_values)\n",
    "\n",
    "# 예측 데이터와 원본 데이터를 합친다. \n",
    "X_with_imputed=np.hstack((imputed_values.reshape(-1,1),X_with_nan[:,1:]))\n",
    "print(X_with_imputed)\n",
    "\n",
    "# 결측치를 대체한 데이터와 훈련에 사용한 데이터 합치기/ \n",
    "result=np.vstack((X_with_imputed,X))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  nan  0.87  0.31]\n",
      " [  nan -0.67  0.22]\n",
      " [ 0.    2.1   1.  ]\n",
      " [ 1.    1.18  1.34]\n",
      " [ 0.    1.21  2.  ]\n",
      " [ 1.    2.    1.34]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.87,  0.31],\n",
       "       [ 0.  , -0.67,  0.22],\n",
       "       [ 0.  ,  2.1 ,  1.  ],\n",
       "       [ 1.  ,  1.18,  1.34],\n",
       "       [ 0.  ,  1.21,  2.  ],\n",
       "       [ 1.  ,  2.  ,  1.34]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가장 많이 나오는 데이터로 대체\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X_complete=np.vstack((X_with_nan,X))\n",
    "print(X_complete)\n",
    "imputer=SimpleImputer(strategy='most_frequent') # 최빈값 대체.\n",
    "imputer.fit_transform(X_complete)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 데이터 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규 표현식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='1'>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 매칭여부를 확인\n",
    "match=re.match('[0-9]','1234')\n",
    "# 패턴에 일치하는 데이터가 있으면 match 객체를 리턴하고 없다면 none 리턴\n",
    "print(match)\n",
    "\n",
    "match=re.match('[0-9]','abc')\n",
    "print(match)# 매치 데이터가 없어 none 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match=' 1'>\n"
     ]
    }
   ],
   "source": [
    "match=re.match('[0-9]',' 1234') # 공백이 있다면 매치를 못한다. \n",
    "match2=re.match('\\s[0-9]',' 1234') # 공백이 있다면 매치를 못한다. -> 공백 제거 추가.  \n",
    "print(match2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@안녕하세요 반갑습니다^^  의미없는 숫자 .....!!!!\n",
      " 안녕하세요 반갑습니다 의미없는 숫자 \n"
     ]
    }
   ],
   "source": [
    "string=\"@안녕하세요 반갑습니다^^ 123 의미없는 숫자 .....!!!!\"\n",
    "\n",
    "# 숫자 데이터 제거\n",
    "p=re.compile('[0-9]+')\n",
    "result=p.sub('',string)\n",
    "print(result)\n",
    "\n",
    "p=re.compile(\"\\W+\")\n",
    "result=p.sub(' ',result)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['안녕하세요 반갑습니다', 'My job is Programmer', 'CC++ C Python']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "text_data=['안녕하세요 반갑습니다.','My job is Programmer','C&C++ ,C#, Python']\n",
    "\n",
    "# 구두점 딕셔너리를 생성\n",
    "punctuation=dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "result=[string.translate(punctuation) for string in text_data]\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kimjimin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kimjimin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오펜하이머는', '얼마나', '좋았을까', '.', '남들', '못가는', '하버드', '대학교도', '가고', '.']\n",
      "['오펜하이머는 얼마나 좋았을까.', '남들 못가는 하버드 대학교도 가고.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize # 단어단위 토큰화\n",
    "from nltk.tokenize import sent_tokenize # 문장단위 토큰화\n",
    "\n",
    "string=\"오펜하이머는 얼마나 좋았을까. 남들 못가는 하버드 대학교도 가고.\"\n",
    "print(word_tokenize(string))\n",
    "\n",
    "print(sent_tokenize(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1월', '3월', '4월']\n",
      "['chief', 'president', 'kenedy', 'move']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# 불용어 제거\n",
    "word_Korean=['1월','2월','3월','4월']\n",
    "stopword=['2월','5월']\n",
    "# i for i in word_Korean if i not in stopwords 는 작업을 수행해서 generator 를 생성\n",
    "# generator 는 iterator 로 접근할 수 있는 객체\n",
    "result=[i for i in word_Korean if i not in stopword]\n",
    "print(result)\n",
    "\n",
    "# 영문은 nltk 에서 기본적인 불용어 사전을 제공. \n",
    "word_eng=['chief','the','an','and','president','kenedy','move']\n",
    "result=[w for w in word_eng if not w in stopwords.words('english')]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['chief', 'president', 'kenedy']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "result=[w for w in word_eng if not w in ENGLISH_STOP_WORDS]\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'pythoners', 'have', 'pythoned', 'poorly', 'at', 'least', 'once']\n",
      "['all', 'python', 'have', 'python', 'poorli', 'at', 'least', 'onc']\n",
      "['al', 'python', 'hav', 'python', 'poor', 'at', 'least', 'ont']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "string=\"All pythoners have pythoned poorly at least once\"\n",
    "\n",
    "words=word_tokenize(string)\n",
    "\n",
    "ps_stemmer=PorterStemmer()\n",
    "print(words)\n",
    "\n",
    "\n",
    "result=[ps_stemmer.stem(i) for i in words]\n",
    "print(result)\n",
    "\n",
    "LC_stemmer=LancasterStemmer()\n",
    "result=[LC_stemmer.stem(i) for i in words]\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hi', 'NN'), ('!', '.'), (',', ','), ('my', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('jason', 'JJ'), ('Kim', 'NNP'), ('.', '.'), ('I', 'PRP'), (\"'m\", 'VBP'), ('Interested', 'JJ'), ('in', 'IN'), ('NLP', 'NNP')]\n",
      "['Hi', 'name', 'Kim', 'NLP']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/kimjimin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "tokens=word_tokenize(\"Hi!, my name is jason Kim. I'm Interested in NLP\")\n",
    "tags_en=pos_tag(tokens)\n",
    "print(tags_en) # 단어와 품사의 튜플 리스트로 출력. \n",
    "# 품사 ( NNP- 고유명사 , NN- 명사 , RB- 부사, VBD- 동사 , VBG- 동사 , 동명사,현재분사, JJ- 형용사)\\\n",
    "print([word for word,tag in tags_en if tag in ['NN','NNP']])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 분석\n",
    "* 성능은 kkma 가 우수하다고 알려져 있지만 메모리 사용량이 많고 속도가 조금 느리다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Hannanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['김한 경은 1998년 6월 29일 생으로 현재 당산 역 앞 삼성 레 미안 아파트에서 거주하고 있다.']\n",
      "['김', '경', '1998', '1998년', '년', '6', '6월', '월', '29', '29일', '일', '현재', '당산', '당산역', '역', '앞', '삼성', '레', '레미안', '미안', '아파트', '거주']\n",
      "[('김', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('경', 'NNG'), ('은', 'JX'), ('1998', 'NR'), ('년', 'NNM'), ('6', 'NR'), ('월', 'NNM'), ('29', 'NR'), ('일', 'NNM'), ('생으로', 'MAG'), ('현재', 'NNG'), ('당산', 'NNP'), ('역', 'NNG'), ('앞', 'NNG'), ('삼성', 'NNG'), ('레', 'NNG'), ('미안', 'NNG'), ('아파트', 'NNG'), ('에서', 'JKM'), ('거주', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), ('있', 'VXV'), ('다', 'EFN'), ('.', 'SF')]\n"
     ]
    }
   ],
   "source": [
    "text=\"김한경은 1998년 6월 29일 생으로 현재 당산역 앞 삼성 레미안 아파트에서 거주하고 있다. \"\n",
    "kkma=Kkma()\n",
    "# 문장 분석 \n",
    "print(kkma.sentences(text))\n",
    "# 단어 분석\n",
    "print(kkma.nouns(text))\n",
    "# 형태소 분석 \n",
    "print(kkma.pos(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('김한경', 'N'), ('은', 'J'), ('1998년', 'N'), ('6월', 'N'), ('29일', 'N'), ('생', 'N'), ('으로', 'J'), ('현재', 'M'), ('당산역', 'N'), ('앞', 'N'), ('삼성', 'N'), ('레미안', 'N'), ('아파트', 'N'), ('에서', 'J'), ('거주', 'N'), ('하고', 'J'), ('있', 'P'), ('다', 'E'), ('.', 'S')]\n",
      "['김한경', '1998년', '6월', '29일', '생', '당산역', '앞', '삼성', '레미안', '아파트', '거주']\n"
     ]
    }
   ],
   "source": [
    "han=Hannanum()\n",
    "# 형태소 분석\n",
    "print(han.pos(text))\n",
    "# 단어 분석\n",
    "print(han.nouns(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW (bag of words - 단어의 개수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barcelona' 'gemany' 'hate' 'kim' 'korea' 'love' 'madrid' 'minjae' 'real']\n",
      "  (0, 5)\t1\n",
      "  (0, 4)\t2\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 6)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 0)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 3)\t1\n",
      "[[0 0 0 0 2 1 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 1]\n",
      " [1 0 1 0 0 0 0 0 0]\n",
      " [0 1 0 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data=np.array(['I love Korea korea','I love real Madrid','I hate Barcelona','Gemany minjae kim'])\n",
    "co=CountVectorizer()\n",
    "bag_of_words=co.fit_transform(text_data)\n",
    "\n",
    "print(co.get_feature_names_out())\n",
    "# 희소행렬의 형태로 출력됨. \n",
    "print(bag_of_words)\n",
    "# 밀집 행렬의 형태로 변경\n",
    "print(bag_of_words.toarray())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### td-idf ( 단어의 가중치 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['barcelona' 'gemany' 'hate' 'kim' 'love' 'madrid' 'minjae' 'real']\n",
      "  (0, 4)\t0.5773502691896257\n",
      "  (0, 5)\t0.5773502691896257\n",
      "  (0, 7)\t0.5773502691896257\n",
      "  (1, 4)\t0.5773502691896257\n",
      "  (1, 5)\t0.5773502691896257\n",
      "  (1, 7)\t0.5773502691896257\n",
      "  (2, 0)\t0.7071067811865476\n",
      "  (2, 2)\t0.7071067811865476\n",
      "  (3, 3)\t0.5773502691896257\n",
      "  (3, 6)\t0.5773502691896257\n",
      "  (3, 1)\t0.5773502691896257\n",
      "{'real': 7, 'madrid': 5, 'love': 4, 'hate': 2, 'barcelona': 0, 'gemany': 1, 'minjae': 6, 'kim': 3}\n",
      "[[0.         0.         0.         0.         0.57735027 0.57735027\n",
      "  0.         0.57735027]\n",
      " [0.         0.         0.         0.         0.57735027 0.57735027\n",
      "  0.         0.57735027]\n",
      " [0.70710678 0.         0.70710678 0.         0.         0.\n",
      "  0.         0.        ]\n",
      " [0.         0.57735027 0.         0.57735027 0.         0.\n",
      "  0.57735027 0.        ]]\n",
      "          0        1         2        3        4        5        6        7\n",
      "0  0.000000  0.00000  0.000000  0.00000  0.57735  0.57735  0.00000  0.57735\n",
      "1  0.000000  0.00000  0.000000  0.00000  0.57735  0.57735  0.00000  0.57735\n",
      "2  0.707107  0.00000  0.707107  0.00000  0.00000  0.00000  0.00000  0.00000\n",
      "3  0.000000  0.57735  0.000000  0.57735  0.00000  0.00000  0.57735  0.00000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data=np.array(['I real Madrid love','I love real Madrid','I hate Barcelona','Gemany minjae kim'])\n",
    "tf=TfidfVectorizer()\n",
    "feature_matrix=tf.fit_transform(text_data)\n",
    "\n",
    "print(tf.get_feature_names_out())\n",
    "# 희소행렬의 형태로 출력됨. \n",
    "print(feature_matrix)\n",
    "# 위치에 해당하는 단어가 나온다.\n",
    "print(tf.vocabulary_)\n",
    "# 밀집 행렬의 형태로 변경\n",
    "print(feature_matrix.toarray())\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(feature_matrix.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
